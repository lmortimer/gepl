from langchain import PromptTemplate
from langchain.prompts.chat import (
    SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatPromptTemplate,
)
from langchain.schema import BaseMessage, HumanMessage


def create_generate_code_prompt(message: str) -> list[BaseMessage]:
    """
    A HumanMessage to prompt the LLM to generate code
    """
    return [HumanMessage(content=message)]


def create_generated_code_executed_prompt(message: str, code: str, result: str) -> list[BaseMessage]:
    """
    A System prompt to record when the user has executed code generated by the LLM.

    Essentially this feeds REPL history generated by the LLM back into the LLM for the next round of generation.
    """

    template = """
    Previously the user asked you {message} and you generated code {code}. 
    Do not run this code again. 
    This code was evaluated in a python3 interpreter and returned {result}
    """

    return \
        SystemMessagePromptTemplate(
            prompt=PromptTemplate(
                template=template,
                input_variables=["message", "code", "result"])).format_messages(message=message, code=code,
                                                                                result=result)


def create_user_executed_code_prompt(code: str, result: str) -> list[BaseMessage]:
    """
    A System prompt to record when the user has executed code that they wrote.

    Essentially this feeds REPL history generated by the user into the LLM for the next round of generation.
    """

    template = """
    The user has executed code. 
    This is the code that was executed:
    
     {code}
     
    Do not run this code again. Remember the symbols, functions, and variables it defines.
    This code was evaluated in a python3 interpreter and returned 
    
    {result}
    """

    return \
        SystemMessagePromptTemplate(
            prompt=PromptTemplate(
                template=template,
                input_variables=["code", "result"])).format_messages(code=code, result=result)


initial_prompt_message = """
You are a python code generator. Write well-written python 3 code. 

The code you generate will be fed into a REPL. Some code and symbols may already be defined by the user.

If you cannot return executable python code set the reason why in the description and return no code.

If you generate a function do not call it.

Return executable python3 code and a description of what the code does in the format:

STARTDESC description ENDDESC

STARTCODE code ENDCODE
"""

# prompts used for model bootstrapping
_initial_system_prompt = SystemMessagePromptTemplate.from_template(initial_prompt_message)

_example_human = HumanMessagePromptTemplate.from_template("print the string hello world")
_example_ai = AIMessagePromptTemplate.from_template("STARTDESC prints the string hello world ENDDESC STARTCODE print('hello world') ENDCODE")
_human_message_prompt = HumanMessagePromptTemplate.from_template("{text}")

STARTUP_PROMPT = ChatPromptTemplate.from_messages(
    [_initial_system_prompt, _example_human, _example_ai, _human_message_prompt])
